{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_urls(url_main, total_pages):\n",
    "    \"\"\"\n",
    "    Mengambil URL produk dari kategori dengan jumlah halaman yang ditentukan secara manual menggunakan Selenium.\n",
    "    \"\"\"\n",
    "    \n",
    "    product_urls = []\n",
    "    base_url = url_main.split('?')[0]  # Ambil base URL tanpa parameter\n",
    "\n",
    "    # Inisialisasi Chrome WebDriver\n",
    "    driver = webdriver.Chrome()  # Pastikan chromedriver sudah di-setup di PATH\n",
    "\n",
    "    try:\n",
    "        for page in range(1, total_pages + 1):\n",
    "            url_with_page = f\"{base_url}?ob=5&page={page}\"\n",
    "            print(f\"Processing page {page}/{total_pages}: {url_with_page}\")\n",
    "            \n",
    "            # Akses URL menggunakan WebDriver\n",
    "            driver.get(url_with_page)\n",
    "            sleep(3)\n",
    "\n",
    "            for _ in range(20):\n",
    "                driver.execute_script(\"window.scrollBy(0, 250)\")\n",
    "                sleep(1)\n",
    "\n",
    "            # Ambil halaman HTML setelah scrolling selesai\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            # Cari elemen produk berdasarkan class\n",
    "            product_divs = soup.find_all('div', {\"class\": \"css-bk6tzz e1nlzfl2\"})\n",
    "            \n",
    "            if not product_divs:\n",
    "                print(f\"  No products found on page {page}. Stopping scraping for this category.\")\n",
    "                break\n",
    "\n",
    "            for div in product_divs:\n",
    "                a_tag = div.find('a')\n",
    "                if a_tag and a_tag.get('href'):\n",
    "                    href = a_tag['href']\n",
    "                    if \"https://ta.tokopedia.com/promo/v1/clicks/\" not in href:\n",
    "                        product_urls.append(href)\n",
    "                        print(f\"URL added: {href}\")\n",
    "                    else:\n",
    "                        print(f\"Skipping promo URL: {href}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    print(f\"Total product URLs collected: {len(product_urls)}\")\n",
    "    return product_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reviews_and_ratings(product_urls):\n",
    "    \n",
    "    reviews = []\n",
    "    ratings = []\n",
    "    \n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    try:\n",
    "        for product_url in product_urls:\n",
    "            print(f\"Processing: {product_url}\")\n",
    "            driver.get(product_url)\n",
    "\n",
    "            while True:\n",
    "                # Scroll the page to load all reviews\n",
    "                for _ in range(20):\n",
    "                    driver.execute_script(\"window.scrollBy(0, 250)\")\n",
    "                    sleep(1)\n",
    "\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "                # Extract reviews and ratings\n",
    "                for product in soup.find_all('div', {\"class\": \"css-1k41fl7\"}):\n",
    "                    review_element = product.find('span', {\"data-testid\": \"lblItemUlasan\"})\n",
    "                    reviews.append(review_element.get_text() if review_element else 'None')\n",
    "\n",
    "                    rating_element = product.find('div', {\"class\": \"rating\"})\n",
    "                    ratings.append(rating_element.get('aria-label') if rating_element else 'None')\n",
    "\n",
    "                # Check if \"Next\" button exists and is enabled\n",
    "                try:\n",
    "                    next_button_container = driver.find_element(By.CLASS_NAME, \"css-1xqkwi8\")\n",
    "                    next_button = next_button_container.find_element(\n",
    "                        By.XPATH, './/button[contains(@class, \"css-16uzo3v-unf-pagination-item\") and @aria-label=\"Laman berikutnya\"]'\n",
    "                    )\n",
    "                    is_disabled = next_button.get_attribute(\"disabled\")  # Check if button is disabled\n",
    "                    if is_disabled:\n",
    "                        print(\"No more pages to navigate for this product.\")\n",
    "                        break\n",
    "\n",
    "                    # Scroll to and click the next button\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_button)\n",
    "                    driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                    sleep(2)\n",
    "                except (NoSuchElementException, TimeoutException):\n",
    "                    print(\"No 'Next' button found. Moving to next product.\")\n",
    "                    break\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    print(f\"Scraped {len(reviews)} reviews and {len(ratings)} ratings.\")\n",
    "    data = pd.DataFrame({'Review': reviews, 'Rating': ratings})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(url_main):\n",
    "\n",
    "    for url in url_main:\n",
    "        product_urls = get_product_urls(url,105)#ganti jumlah halaman\n",
    "\n",
    "    # Scrape reviews and ratings for each product\n",
    "    data = scrape_reviews_and_ratings(product_urls)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlss = ['https://www.tokopedia.com/p/fashion-pria/sepatu-pria?page=1&rt=1,2,3&ob=5'] #ganti link sendiri\n",
    "\n",
    "data = main(urlss)\n",
    "data.to_csv(\"reviewsnrating123.csv\", index=False)#ganti nama csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
